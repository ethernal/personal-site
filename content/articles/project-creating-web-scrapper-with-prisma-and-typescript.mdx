---
title: Web scrapper with prisma and typescript with custom frontend
author: Sebastian Pieczynski
publishedOn: 2023-11-23
published: true
status: published
series: web-scrapper-2023
seriesPart: 1
abstract: How to create a web scrapper with prisma and typescript with custom frontend and simple backed using Vite
category: React
keywords: [react, web scrapper, crawler, cheerio, axios, rest, cors, prisma, typescript, vite]
image: "/image/article/why-do-i-need-key-when-mapping-through-list-of-items-in-react/key-in-react-map.jpeg"
imageAlt: "pirates loading treasure chests from tropical island onto the ship, huge hole near the chests, some chests are still buried in the sand, some open treasure chests filled with gold are visible on the ship, view from the beach to the ship, day, warm, cumulus clouds, watercolor"
---

## Contents

<p className="text-3xl text-theme-accent font-semibold font-heading">How arrrr... ya Engineers? ü¶ú</p>

Seems to me we have found ourselves a mighty trrr...easure.. We just need to dig it up.

Stop dilly dallying and let's get to work. Today we are digging data from the web pages and claiming it as our treasure.

This is a two part article. In the first we will build full scrapper from start to finish and in the second we'll take care of the backend and frontend.

## Acknowledgements

Note to people who do this daily - please send your suggestions how to improve the crawler. It is my first attempt at such a project and it is not as efficient as it could be (no concurrency) but was a fun to make and if possible I would also love to learn more from You.

Finished implementation is available at: [GitHub](https://github.com/ethernal/web-scrapper-demo) <NewTabIcon/>

The original and not a curated repository so you will be able to see the thought process and what went right and what needed changing in the course of building it.

REST API will be very limited as it's not a proper implementation of a backend server.

The initial scrapper implementation was inspired by: https://www.zenrows.com/blog/javascript-web-crawler-nodejs#create-web-crawler-in-node


With that in mind let's get started.

## What we're making

To dig that treasure üèùÔ∏è we'll need some tools:

1. Web scrapper that will get the data from the external website,
2. it will save it to a local sqlite database,
3. and download all the images associated with the products (so we're not overloading the site too much),
4. modify data on the fly to adjust it for the frontend.
5. Then a "REST API" that will serve the data from the database.
6. Finally a frontend that will fetch the data from our "REST" server and display it to the user,
7. it will also allow filtering and sorting the products,
8. and state of the frontend app will be controlled with search parameters.

## Building a shovel

We have to get data for all the products listed on a certain website. We need to get the following information for the product:

- name
- price
- currency (if it can be extracted)
- image
- link

For the source of data we'll be using the [~Pokemon Shop~ ScrapeMe shop site](https://scrapeme.live/shop) that has all the products listed on ~50 pages.

To get the data we'll use the [Axios](https://github.com/axios/axios) and to query the DOM of the fetched pages we'll use the [CheerioJS](https://github.com/cheeriojs/cheerio) libraries.

Since we want to use both Typescript and ESM modules we'll use [tsx](https://github.com/privatenumber/tsx) package to run our scrapper server.

To start clone the [initial version of the repo](https://github.com/ethernal/web-scrapper-demo/tree/init) <NewTabIcon/>.

```bash
git clone --bare https://github.com/ethernal/web-scrapper-demo/tree/init
```

Create a `scrapper.ts` file in the root of the project and import both `axios` and `cheerio` and create a main function that will run the scrapper:

````scrapper.ts
import axios from 'axios';
import cheerio from 'cheerio';

async function main(maxPages = 5) {
  console.log(`Hello World. Scraping ${maxPages} pages.`);
}

main()
    .then(() => {
        process.exit(0);
    })
    .catch((e) => {
        // logging the error message
        console.error(e);

        process.exit(1);
    });
````

Then in the `package.json` file inside "scripts" section add the following:

```json
"scripts": {
  //...
  "scrap": "tsx scrapper.ts"
  //...
}
```

Running the script should return

```bash
  Hello World. Scraping 5 pages.
```

If you have issues at this stage you can open an issue on [GitHub](https://github.com/ethernal/web-scrapper-demo/issues/new?labels=&template=issue.md&title=Step%201%20running%20tsx%20hello%20world) <NewTabIcon /> or [contact me directly](/contact).

The `maxPages` parameter will be important to control the number of pages to crawl.

For index of visited pages we'll use a `Set`. This structure is simillar to an array but it does not allow duplicates so it will be perfect to keep track of what page was visited. Even if we try to add the same page a second time `Set` will not add the same value again. Read more about Sets on MDN [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set) <NewTabIcon />.

Inside `main` function we'll first look for all the pages that we can crawl and add the links to the queue. Then we'll visit each page (download with Axios) and check for the existence of data we need in the HTML content. Let's dive into the code and I'll guide you with the comments alongside it.

Well first check navigation for all pages and add them to the crawl list:

<div className="margin-auto w-full mb-4">
<Image src="/image/article/web-crawler/navigation-url-list-selector.png" alt="Navigation URLs accessible with '.page-numbers a' selector" width={1300} height={300}/>
</div>

The image shows that it is possible to access the link for a page with `.page-numbers a` selector.

Now for the actual product data we can use the `li.product a.woocommerce-LoopProduct-link` selector:

<div className="margin-auto w-full mb-4">
<Image src="/image/article/web-crawler/product-data-selector.png" alt="Product data in HTML accessible with 'li.product a.woocommerce-LoopProduct-link' selector" width={1300} height={300}/>
</div>

The keen eyed will also note that there is a lot more data there that we are interested in.

With that in mind let's start.

```scrapper.ts
async function main(maxPages = 5) {

    // start with the initial webpage to visit
    const paginationURLsToVisit = ["https://scrapeme.live/shop"];

    // list of URLs the crawler has visited
    const visitedURLs:Set<string> = new Set();

    const productURLs = new Set();

    // iterating until the queue is empty
    // or the iteration limit is hit
    while (
        paginationURLsToVisit.length !== 0 &&
        visitedURLs.size <= maxPages
        ) {
        // the current url to crawl
        const paginationURL = paginationURLsToVisit.pop();

        // if the queue is empty, break the loop
        if (paginationURL === undefined) {
            break;
        }

        // retrieving the HTML content of the page from paginationURL
        const pageHTML = await axios.get(paginationURL);

        // adding the current webpage to the set of
        // web pages already crawled
        visitedURLs.add(paginationURL);

        // initializing cheerio on the current webpage
        const $ = cheerio.load(pageHTML.data);

        // get all pagination URLs and for each page...
        // see image above
        $(".page-numbers a").each((index, element) => {
            const paginationURL = $(element).attr("href");

            // if the queue is empty, break the loop
            if (paginationURL === undefined) {
                return;
            }
            // adding the pagination URL to the queue
            // of web pages to crawl, if it wasn't yet crawled
            if (
                !visitedURLs.has(paginationURL) &&
                !paginationURLsToVisit.includes(paginationURL)
            ) {
                paginationURLsToVisit.push(paginationURL);
            }
        });

        // retrieve the product URLs
        $("li.product a.woocommerce-LoopProduct-link").each((index, element) => {
            const productURL = $(element).attr("href");
            productURLs.add(productURL);
        });
    }
    //...
    console.log([...productURLs]);
}
```

Running this code should return an array of product URLs, but since it's just logged to the console we cannot do much with them.

For now our scrapper doesn't do much but there is everything in place for us to get all that shiny data we so desire.

For reference here's the full source of the initial scrapper:

```scrapper.ts
import axios from 'axios';
import cheerio from 'cheerio';

// limit the demo use to 5 pages instead of 50
async function main(maxPages = 5) {
    // initialized with the initial webpage to visit
    const paginationURLsToVisit = ["https://scrapeme.live/shop"];
    const visitedURLs:Set<string> = [];

    const productURLs = new Set();

    // iterating until the queue is empty
    // or the iteration limit is hit
    while (
        paginationURLsToVisit.length !== 0 &&
        visitedURLs.size <= maxPages
        ) {
        // the current url to crawl
        const paginationURL = paginationURLsToVisit.pop();

        // if the queue is empty, break the loop
        if (paginationURL === undefined) {
            break;
        }

        // retrieving the HTML content from paginationURL
        const pageHTML = await axios.get(paginationURL);

        // adding the current webpage to the
        // web pages already crawled
        visitedURLs.add(paginationURL);

        // initializing cheerio on the current webpage
        const $ = cheerio.load(pageHTML.data);

        // retrieving the pagination URLs
        $(".page-numbers a").each((index, element) => {
            const paginationURL = $(element).attr("href");

            // if the queue is empty, break the loop
            if (paginationURL === undefined) {
                return;
            }
            // adding the pagination URL to the queue
            // of web pages to crawl, if it wasn't yet crawled
            if (
                !visitedURLs.has(paginationURL) &&
                !paginationURLsToVisit.includes(paginationURL)
            ) {
                paginationURLsToVisit.push(paginationURL);
            }
        });

        // retrieving the product URLs
        $("li.product a.woocommerce-LoopProduct-link").each((index, element) => {
            const productURL = $(element).attr("href");
            productURLs.add(productURL);
            console.log(`Added: ${productURL}`);
        });
    }

    // logging the crawling results
    console.log([...productURLs]);

    // use productURLs for scraping purposes...
}

main()
    .then(() => {
        process.exit(0);
    })
    .catch((e) => {
        // logging the error message
        console.error(e);

        process.exit(1);
    });
```

## Extracting more data and saving to local database

Now we will add database connectivity to our scrapper, this will allow us to process the data later without accessing the site again.

First we need to add prisma to our project as a dependency:

```bash
npm install prisma --save-dev
```

and then initialize the database connection:

```bash
npx prisma init --datasource-provider sqlite
```

This will create a new `prisma` folder with `schema.prisma` file inside. We'll also keep a DATABASE URL in `.env` file.

```schema.prisma
generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "sqlite"
  url      = env("DATABASE_URL")
}
```

Before continuing make sure that in `.gitignore` `.env` files are set to be omitted, BUT not `.env.dist`.
You can exclude a file pattern in `.gitignore` by adding an `!` before the pattern or file name.

```.gitignore
# local env files
.env*.local
.env
.env.*
!.env.dist
```

Create two files: `.env` and `.env.dist`. We will use `.env` file as our source of truth and `.env.dist` as a sample or reference file.

When you clone the full repository `.env` file will not be present so `.env.dist` will tell you what environmental variables you need to set to successfully run the project.

In the `.env` file add the following:

```.env
DATABASE_URL="file:./data/dev.db"
```

It will create a `/data/dev.db` file and subfolder inside `/prisma` folder.

Now we need to design our database schema. We'll keep it simple but fairly robust for the use case. We'll assume the most important aspect for our users is the price of the product but we'll keep all additional information in the database as JSON.

Again `prisma` does not support `JSON` field type for `sqlite` so we'll use a string and parse it when necessary.

We'll also have the `User` model to test our database connection.


```prisma.schema
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "sqlite"
  url      = env("DATABASE_URL")
}

model User {
  id        String @id @default(cuid())
  email     String @unique
  firstName String
  lastName  String
}

model ScrappedData {
  id        String   @id @default(cuid())
  url       String   @unique
  price     Float
  data      String // serialize and deserialize from string until support is added in prisma
  dataType  String   @default("product")
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt
}
```

Points to note about the schema:

1. `id` field is auto-generated and unique and uses CUID as value of an id field.
2. `createdAt` and `updatedAt` fields are auto-generated.
3. We make sure that data is collected only once for a given `url` so it will be a unique field.
3. `data` will hold any data we extract and do not need to directly operate on (ex. filter).
4. `dataType` is there to distinguish between different types of data we could obtain and will not be used in our examples, it's there to give you an idea how to expand the scrapper capabilities ex. templates.

And now for a real **gem** of this article:

It is possible to SEED prisma database with seed script written in typescript and using ESM imports!

Try it in a project without `tsx` installed. I have, it's not fun..

Add a prisma seed script to the `package.json` file. It's a "standalone" configuration that does **not** belong in "scripts" section.:

```package.json
{
  //...
  "type": "module",
  "scripts": {
    "dev": "vite",
    "scrap": "tsx scrapper.ts",
    //...
  },
  "prisma": {
    "seed": "tsx prisma/seed.ts"
  },
  "dependencies": {
    //...
  }
  //...
}
```

And now let's create the `seed.ts` file:

```seed.ts
// prisma/seed.ts
import { PrismaClient } from '@prisma/client';

const prisma = new PrismaClient();

async function main() {
    // we're using upsert to make sure we're not trying to insert same data twice (it will error out)
	await prisma.user.upsert({
		where: { email: `admin@example.com` },
		create: {
			email: `admin@example.com`,
			firstName: 'Admin',
			lastName: 'Super',
		},
		update: {
			firstName: 'Admin',
			lastName: 'Super',
		},
	});
}

console.log('Seeding the database');
main()
	.catch((e) => {
		console.error(e);
		process.exit(1);
	})
	.finally(async () => {
	    await prisma.$disconnect();
        console.log('Completed seeding the database');
	});
```

Run the code with:

```bash
npx prisma db seed
```

`npx` will run the `prisma` script from node_modules and execute the `seed.ts` with `tsx`. It should return in the console:

```bash
Seeding the database
Completed seeding the database
```

Our database connection works and the seed script can be run multiple times thanks to using `upsert`, you can try again.

> A side note to Next.js developers reading this: this method also works with Next.js! Tested and confirmed. You're welcome!

Now that we have our database connected and confirmed it works let's add the same capability to the scrapper. We'll also extract more information about the product and save it to the database.

```scrapper.ts
import axios from 'axios';
import cheerio from 'cheerio';

import { PrismaClient } from '@prisma/client';

async function main(maxPages = 50) {
    const prisma = new PrismaClient();
    // initialized with the webpage to visit
    const paginationURLsToVisit = ["https://scrapeme.live/shop"];
    const visitedURLs:Set<string> = new Set();

    const products = new Set();

    // iterating until the queue is empty
    // or the iteration limit is hit
    while (
        paginationURLsToVisit.length !== 0 &&
        visitedURLs.size <= maxPages
        ) {
        // the current url to crawl
        const paginationURL = paginationURLsToVisit.pop();

        // if the queue is empty, skip the current iteration and continue the loop
        if (paginationURL === undefined) {
            continue;
        }

        // retrieving the HTML content from paginationURL
        const pageHTML = await axios.get(paginationURL);

        // adding the current webpage to the
        // web pages already crawled
        visitedURLs.add(paginationURL);

        // initializing cheerio on the current webpage
        const $ = cheerio.load(pageHTML.data);

        // retrieving the pagination URLs
        $(".page-numbers a").each((index, element) => {
            const paginationURL = $(element).attr("href");

            // if the queue is empty, skip to the next element in the loop
            if (paginationURL === undefined) {
                return;
            }
            // adding the pagination URL to the queue
            // of web pages to crawl, if it wasn't yet crawled
            if (
                !visitedURLs.has(paginationURL) &&
                !paginationURLsToVisit.includes(paginationURL)
            ) {
                paginationURLsToVisit.push(paginationURL);
            }
        });

        console.log('Adding products...');
        // retrieving the product URLs
        $("li.product a.woocommerce-LoopProduct-link").each((index, element) => {
            // extract all information about the product
            const productURL = $(element).attr("href");
            const productImg = $(element).find("img").attr("src");
            const productName = $(element).find("h2").text();
            const productPrice = $(element).find(".woocommerce-Price-amount").text();
            const productPriceCurrency = $(element).find(".woocommerce-Price-currencySymbol").text();

            const product = {
                name: productName,
                price: productPrice.replaceAll(productPriceCurrency, ""), // remove currency symbol from the price
                currency: productPriceCurrency,
                image: productImg,
                url: productURL
            }

            products.add(product);

            if (productURL === undefined) {
                return;
            }

            // create a function that will save the information about the product to the database
            const addData = async (data: typeof product) => {
                // use upsert to create row if it does not exist or update data if it has changed since last run
                // sqlite and prisma don't support createMany so we need to use per element inserts
                await prisma.scrappedData.upsert({
                    where: {
                        url: data.url,
                    },
                    create: {
                        url: data.url,
                        price: parseFloat(data.price),
                        data: JSON.stringify(data),
                    },
                    update: {
                        price: parseFloat(data.price),
                        data: JSON.stringify(data),
                    }
                })
            }

            // Here we're saving scrapped data to the database
            addData(product);

            console.log(`Added: ${JSON.stringify(product, undefined, 2)}`);
        });
    }

    console.log('Products added.');
}

main()
    .then(() => {
        process.exit(0);
    })
    .catch((e) => {
        // logging the error message
        console.error(e);

        process.exit(1);
    });
```

As you can see not much was really needed in the scrapper code itself to make it work.

We have added a function that saves the product data to the database and extracted information about the product from the website. The function for prisma works the same way as the seeding function, but if you do have questions please [contact me](/contact).

```scrapper.ts
const productURL = $(element).attr("href");
const productImg = $(element).find("img").attr("src");
const productName = $(element).find("h2").text();
const productPrice = $(element).find(".woocommerce-Price-amount").text();
const productPriceCurrency = $(element).find(".woocommerce-Price-currencySymbol").text();
```

If you run the scrapper now you should see the products in the database. You can use [HeidiSQL](https://www.heidisql.com/download.php) to view the data in the database.

Now there is one last thing we need to do. If you check inside the database you'll see that the images are linked from the original site. This is not good on many levels: we might be overusing the bandwidth or external site and these images may be removed at some point. Let's try limiting our impact on the site and download the images to our folder and serve them on our own.

We will use node and axios to download data. First let's create a function that will receive a list of files to download:

```scrapper.tsx
import axios from 'axios';
import cheerio from 'cheerio';
import fs from 'fs';
import path from 'path';

import { prisma } from './src/lib/prisma';

async function downloadFiles(downloadFiles: Array<string>) {
  console.log(`Starting download of ${downloadFiles.length} files. This will take few minutes. Please be patient...`);

  let fileDownloadCount = 0;

  for (const link of downloadFiles) {
    const name = path.basename(link);
    const url = link
    const file = fs.createWriteStream(`./public/images/${name}`)

    const response = await axios({
      url,
      method: 'GET',
      responseType: 'stream'
    });
    response.data.pipe(file);
    file.on('finish',() => {
        file.close();
        fileDownloadCount++;
    })
  }
  console.log(`Finished downloading ${fileDownloadCount} of {downloadFiles.length} files.`);
}

async function main(maxPages = 50) {
//...
}
```

We have imported the `fs` and `path` to make working with local files possible:

```scrapper.ts
import fs from 'fs';
import path from 'path';
```

Then we declare the function that will accept an `Array` of `string`s that will represent links to files we need to download:

```scrapper.ts
async function downloadFiles(downloadFiles: Array<string>) {
//...
}
```
As we loop through all the files with []`for ... of`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...of) loop we'll extract information from the link to get the name of the file. With it we can can set a custom save location for it.

We also save the full url to the oryginal file as a local variable.

Then we create a file stream inside the `./public/images` folder...

```scrapper.ts
const name = path.basename(link);
const url = link
const file = fs.createWriteStream(`./public/images/${name}`)
```

download a file from `url` with *axios*.

```scrapper.ts
const response = await axios({
      url,
      method: 'GET',
      responseType: 'stream'
    });
```

and finally send the data stream to the file we created earlier. After the stream is finished (dried up) we close the file.

We also keep a count of how many files have been downloaded to present that information to the user so we know that all the files were downloaded.

```scrapper.ts
response.data.pipe(file);
    file.on('finish',() => {
        file.close();
        fileDownloadCount++;
        // console.log(`Download of ${name} completed`);
    });
```

We also have to remember to modify the data of the product that will be saved in the database as the image url will have to point to a local image not to one from the oryginal website:

```scrapper.ts
const productURL = $(element).attr("href");
const productImg = $(element).find("img").attr("src");
const productName = $(element).find("h2").text();
const productPrice = $(element).find(".woocommerce-Price-amount").text();
const productPriceCurrency = $(element).find(".woocommerce-Price-currencySymbol").text();

// when using vite relative path files are served from the public folder by default so there is no need to add the folder to the path - it will produce a warning in the console
const localProductImg = (productImg !== undefined) ? `./images/${path.basename(productImg)}` : productImg;

const product = {
    name: productName,
    price: productPrice.replaceAll(productPriceCurrency, ""),
    currency: productPriceCurrency,
    image: localProductImg,
    url: productURL
}
```

With this our scrapper (shovel to unearth the treasures) is completed.

In this part of the series we have:

[x] Created a web scrapper that visits all the subpages automatically.
[x] Extracts the information about the product from the web page.
[x] Modifies that information before it's ready to be saved.
[x] Saves the product information in the database.
[x] Downloads images.

In the next part we'll create a frontend and a backend to serve the data and visually present it to the users.

See you soon!

Below is the full source of the finished `scrapper.ts`.

## Finished scrapper

```scrapper.ts

import axios from 'axios';
import cheerio from 'cheerio';
import fs from 'fs';
import path from 'path';

import { prisma } from './src/lib/prisma';

async function downloadFiles(downloadFiles: Array<string>) {
  console.log(`Starting download of ${downloadFiles.length} files. This will take few minutes. Please be patient...`);

  let fileDownloadCount = 0;

  for (const link of downloadFiles) {
    // console.log("Downloading file: ", path.basename(link));

    const name = path.basename(link);
    const url = link
    const file = fs.createWriteStream(`./public/images/${name}`)

    const response = await axios({
      url,
      method: 'GET',
      responseType: 'stream'
    });
    response.data.pipe(file);
    file.on('finish',() => {
        file.close();
        fileDownloadCount++;
        // console.log(`Download of ${name} completed`);
    })
  }
  console.log(`Finished downloading ${fileDownloadCount} of {downloadFiles.length} files.`);
}

async function main(maxPages = 50) {

    // initialized with the initial webpage to visit
    const paginationURLsToVisit = ["https://scrapeme.live/shop"];
    const visitedURLs:Set<string> = new Set();

    const products = new Set();
    const imageSrc = new Set<string>();

    // iterating until the queue is empty
    // or the iteration limit is hit
    while (
        paginationURLsToVisit.length !== 0 &&
        visitedURLs.size <= maxPages
        ) {
        // the current url to crawl
        const paginationURL = paginationURLsToVisit.pop();

        // if the queue is empty, skip the current iteration and continue the loop
        if (paginationURL === undefined) {
            continue;
        }

        // retrieving the HTML content from paginationURL
        const pageHTML = await axios.get(paginationURL);

        // adding the current webpage to the
        // web pages already crawled
        visitedURLs.add(paginationURL);

        // initializing cheerio on the current webpage
        const $ = cheerio.load(pageHTML.data);

        // retrieving the pagination URLs
        $(".page-numbers a").each((index, element) => {
            const paginationURL = $(element).attr("href");

            // if the queue is empty, skip to the next element in the loop
            if (paginationURL === undefined) {
                return;
            }
            // adding the pagination URL to the queue
            // of web pages to crawl, if it wasn't yet crawled
            if (
                !visitedURLs.has(paginationURL) &&
                !paginationURLsToVisit.includes(paginationURL)
            ) {
                paginationURLsToVisit.push(paginationURL);
            }
        });

        console.log('Adding products from: ', paginationURL);
        // retrieving the product URLs
        $("li.product a.woocommerce-LoopProduct-link").each((index, element) => {
            const productURL = $(element).attr("href");
            const productImg = $(element).find("img").attr("src");
            const productName = $(element).find("h2").text();
            const productPrice = $(element).find(".woocommerce-Price-amount").text();
            const productPriceCurrency = $(element).find(".woocommerce-Price-currencySymbol").text();

            if (productImg !== undefined) {
                imageSrc.add(productImg);
            }

            // when using vite relative path files are served from the public folder by default so there is no need to add the folder to the path - it will produce a warning in the console
            const localProductImg = (productImg !== undefined) ? `./images/${path.basename(productImg)}` : productImg;

            const product = {
                name: productName,
                price: productPrice.replaceAll(productPriceCurrency, ""),
                currency: productPriceCurrency,
                image: localProductImg,
                url: productURL
            }

            products.add(product);

            if (productURL === undefined) {
                return;
            }

            const addData = async (data: typeof product) => {
                await prisma.scrappedData.upsert({
                    where: {
                        url: data.url,
                    },
                    create: {
                        url: data.url,
                        price: parseFloat(data.price),
                        data: JSON.stringify(data),
                    },
                    update: {
                        price: parseFloat(data.price),
                        data: JSON.stringify(data),
                    }
                })
            }

            addData(product);

            // console.log(`Added: ${JSON.stringify(product, undefined, 2)}`);
        });
    }

    // logging the crawling results
    console.log('Products added.');
    console.log('Downloading images...');
    await downloadFiles(Array.from(imageSrc));
    console.log('Done!');

    // use productURLs for scraping purposes...
}

main()
    .then(() => {
        process.exit(0);
    })
    .catch((e) => {
        // logging the error message
        console.error(e);

        process.exit(1);
    });
```

It wasn't as hard as it seems right?

Let me know what you think or send a PR with improvements!

See you next time!
